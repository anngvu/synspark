type: "quiz"
id: "morgan_direct_s3_access_setup"
title: "Morgan's Direct S3 Access Setup"
level: "intermediate-advanced"
meta:
  source: "2022-07-26.txt"
  source_date: "2022-07-26"
  source_timestamps: ["2022-07-26 10:13:43", "2022-07-26 20:55:17"]
  key_concepts: ["S3 integration", "STS credentials", "direct access", "compute optimization"]
  multiple_correct: false
context: |
  **Direct S3 Access Options for Synapse Data**

  While Synapse provides client libraries for data access, computational biologists working with large datasets may need alternative access methods for specific use cases. Understanding when and how to use direct S3 access can optimize computational workflows.
question: "Morgan, a computational biologist, is considering direct S3 access via STS credentials for processing 500GB of genomics data on AWS EC2. The project has sub-folders with local access controls for certain collaborating institutions. What are the key trade-offs they should evaluate before choosing this approach over the standard Synapse Python client?"
answers:
  - text: "Direct S3 access via STS completely preserves Synapse's folder-level permissions and access controls, so there is not much of a tradeoff to eliminate the need to download 500GB locally for processing"
    correct: false
    message: "This is incorrect. STS tokens have a known permissions backdoor: they grant recursive access to all files within the authorized folder, even if sub-folders have more restrictive permissions for collaborating institutions. This is why STS features are used in very particular circumstances only."
    points: 0
  - text: "STS tokens may compromise sub-folder access controls, but eliminate the need to download 500GB locally for processing"
    correct: true
    message: "Correct! This captures the key trade-off: STS tokens have a permissions backdoor that grants recursive access to all files within authorized folders, potentially bypassing access controls for collaborating institutions' data. However, they enable compute directly on S3-hosted data without downloads, saving significant time and storage costs. This is why STS features are used in very particular circumstances only."
    points: 2
  - text: "Direct S3 access provides faster data transfer speeds compared to the Synapse Python client"
    correct: false
    message: "Download speeds are similar since both ultimately access the same S3 infrastructure. The real benefit is avoiding downloads entirely when compute can happen directly on S3, but given the sub-folder access control issues with STS tokens, this approach should only be considered in very particular circumstances."
    points: 0
  - text: "The security risks are minimal since STS credentials expire automatically"
    correct: false
    message: "While STS credentials do expire, this doesn't address the permissions backdoor issue. During their validity period, STS tokens grant recursive access to all files in authorized folders, potentially exposing data from collaborating institutions with restricted sub-folder access. This is why STS is only recommended in very particular circumstances."
    points: 0
details: |
  **When to Use Direct S3 Access with STS Credentials:**

  Common scenarios where computational biologists choose direct S3 access:

  - **Large-scale compute workflows**: Processing hundreds of GB/TB without local downloads
  - **Cloud-native pipelines**: AWS Batch, EMR, or Lambda functions operating directly on S3 data
  - **Streaming analysis**: Reading data directly into memory without intermediate storage
  - **Cost optimization**: Avoiding egress charges and storage costs for temporary local copies
  - **Integration with existing AWS tools**: Leveraging S3-native tools and services

  **Requesting and Configuring STS Credentials:**

  1. **Get credentials from Synapse:**
  ```python
  import synapseclient
  syn = synapseclient.Synapse()
  syn.login()
  credentials = syn.getSTSCredentials(entity="syn12345", permission="read_only")
  ```

  2. **Configure AWS CLI:**
  ```bash
  export AWS_ACCESS_KEY_ID=<credentials['accessKeyId']>
  export AWS_SECRET_ACCESS_KEY=<credentials['secretAccessKey']>
  export AWS_SESSION_TOKEN=<credentials['sessionToken']>

  # Now use AWS CLI directly
  aws s3 cp s3://proddata.sagebase.org/path/to/file ./local_file
  ```

  3. **Configure boto3 in Python:**
  ```python
  import boto3
  s3_client = boto3.client(
      's3',
      aws_access_key_id=credentials['accessKeyId'],
      aws_secret_access_key=credentials['secretAccessKey'],
      aws_session_token=credentials['sessionToken']
  )
  ```

  This approach maintains Synapse permissions while enabling cloud-native compute patterns essential for modern bioinformatics workflows.
allow_retry: true
random_answer_order: true
